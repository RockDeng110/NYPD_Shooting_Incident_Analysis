---
title: "NYPD_Shooting_Incident"
author: "RockDeng"
date: "2024-01-04"
output:
  html_document: default
  pdf_document: default
---


```{r include = FALSE}
# Load libraries
library(tidyverse)
 
```
### 1. Import data and get some basic understanding of the data.
* Load the data from CSV file into R Dataframe.
* Take a glimpse of the raw data.
* My data is downloaded from data.gov, please check here: https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic
```{r import_files}
# Import files
df <- read_csv("NYPD_Shooting_Incident_Data__Historic_.csv")
glimpse(df)
dim(df)
n = dim(df)[1]
n
```

As we can see the output from the above code. This data set has **27312** shooting incidents in total and **21** columns. 

* Next, let's check out the meaning of each column or variable.

Column | Description 
-------|----------
INCIDENT_KEY | Randomly generated persistent ID for each arrest	
OCCUR_DATE   | Exact date of the shooting incident	
OCCUR_TIME   | Exact time of the shooting incident   
BORO         | Borough where the shooting incident occurred        
LOC_OF_OCCUR_DESC  | NA  
PRECINCT           | Precinct where the shooting incident occurred 
JURISDICTION_CODE  | Jurisdiction where the shooting incident occurred.  
LOC_CLASSFCTN_DESC | NA 
LOCATION_DESC      | Location of the shooting incident   
STATISTICAL_MURDER_FLAG | Shooting resulted in the victim’s death which would be counted as a murder
PERP_AGE_GROUP     | Perpetrator’s age within a category   
PERP_SEX           | Perpetrator’s sex description  
PERP_RACE          | Perpetrator’s race description 
VIC_AGE_GROUP      | Victim’s age within a category  
VIC_SEX            | Victim’s sex description 
VIC_RACE           | Victim’s race description  
X_COORD_CD         | Midblock X-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104)   
Y_COORD_CD         | Midblock Y-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104)   
Latitude           | Latitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)  
Longitude          | Longitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)  
Lon_Lat            | Combination of Latitude and Longtitude


#### State the Purpose
Try to find the reasons and key factors that cause the shooting cases, so as to help law enforcement and the government to adjust policies and improve the current situation.



### 2. Tidying and Transforming Data
* Tidying the data involves cleaning up column names, dealing with missing values, and ensuring that the data is in a format suitable for analysis.
* Transformation typically involves filtering data, creating new variables, and summarizing it to prepare for further analysis.

#### 2.1 Convert the columns to convenient data type
* Conver the OCCUR_DATE variable from date string to date data type
```{r}
#glimpse(df)
# Convert OCCUR_DATE from string to date format
df$OCCUR_DATE = as.Date(df$OCCUR_DATE, format = "%m/%d/%Y")
```



#### 2.2 Remove duplicated columns or those won't be userful for modeling
```{r}
# columns before removing 
print(colnames(df))

# Example: Removing columns
df <- df %>%
  select(-INCIDENT_KEY, -LOC_OF_OCCUR_DESC, -LOC_CLASSFCTN_DESC, -LOCATION_DESC, -X_COORD_CD, -Y_COORD_CD, -Latitude, -Longitude, -Lon_Lat)

# columns after removing 
print(colnames(df))
```



#### 2.3 Checkout and handle NA values
```{r}
summary(df)
```
* We can find out that the categorical variables didn't have NA value.
* Do they really have no NA, or the NA values just hide in the strings, let's find out lately.



##### 2.2.1 Checkout NA values in VIC_AGE_GROUP
```{r}
# Convert category columns to factors
unique(df$VIC_AGE_GROUP)

df %>%
  filter(VIC_AGE_GROUP %in% c("UNKNOWN")) %>%
  count() ->
  vic_age_na_count

vic_age_na_count
# Pecentage of na values in this variable
(vic_age_na_count / n) * 100


# remove those UNKNOWN values
df <- subset(df, VIC_AGE_GROUP != "UNKNOWN")

unique(df$VIC_AGE_GROUP)
```

##### 2.2.2  Checkout NA values in VIC_SEX
U should stand for "undefined", let's set it to NA.
```{r}
unique(df$VIC_SEX)
df %>%
  filter(VIC_SEX %in% c("U")) %>%
  count() ->
  vic_sex_na_count

vic_sex_na_count
# Pecentage of na values in this variable
(vic_sex_na_count / n) * 100


# remove those UNKNOWN values
df <- subset(df, VIC_SEX != "U")

unique(df$VIC_SEX)
```

##### 2.2.3  Checkout NA values in VIC_RACE
```{r}
unique(df$VIC_RACE)
df %>%
  filter(VIC_RACE %in% c("UNKNOWN")) %>%
  count() ->
  vic_race_na_count

vic_race_na_count
# Pecentage of na values in this variable
(vic_race_na_count / n) * 100


# remove those UNKNOWN values
df <- subset(df, VIC_RACE != "UNKNOWN")

unique(df$VIC_RACE)
```

##### 2.2.4  Checkout NA values in PERP_AGE_GROUP
```{r}
unique(df$PERP_AGE_GROUP)
df %>%
  filter(PERP_AGE_GROUP %in% c(NA, "UNKNOWN", "(null)")) %>%
  count() ->
  perp_age_na_count

perp_age_na_count
# Pecentage of na values in this variable
(perp_age_na_count / n) * 100

# Too many na values, remove this column
df <- subset(df, select = -PERP_AGE_GROUP)

print(colnames(df))

```
  * The rate of NA value in this variable is really high, and almost half of the observations are NA here.
  * Therefore this variable might not useful for modeling, otherwise we need to remove almost half of the dataset.
  
 
  
##### 2.2.5  Checkout NA values in PERP_SEX
  * Like the previous variable PERP_AGE_GROUP, the rate of NA values in this variable is almost same as the PERP_AGE_GROUP.
```{r}
unique(df$PERP_SEX)
df %>%
  filter(PERP_SEX %in% c(NA, "(null)", "U")) %>%
  count() ->
  perp_sex_na_count
perp_sex_na_count

# Pecentage of na values in this variable
(perp_sex_na_count / n) * 100

# Too many na values, remove this column
df <- subset(df, select = -PERP_SEX)

print(colnames(df))
```

##### 2.2.6  Checkout NA values in PERP_RACE
  * NA rate still almost equal to 0.5 for this variable
```{r}
unique(df$PERP_RACE)
df %>%
  filter(PERP_RACE %in% c(NA, "UNKNOWN", "(null)")) %>%
  count() ->
  perp_race_na_count

perp_race_na_count
# Pecentage of na values in this variable
(perp_race_na_count / n) * 100

# Too many na values, remove this column
df <- subset(df, select = -PERP_RACE)

print(colnames(df))
```
##### 2.2.7  Checkout NA values in PERP_RACE
```{r}
unique(df$JURISDICTION_CODE)
df %>%
  filter(JURISDICTION_CODE %in% c(NA, "UNKNOWN", "(null)")) %>%
  count() ->
  JURISDICTION_CODE_na_count

JURISDICTION_CODE_na_count
# Pecentage of na values in this variable
(JURISDICTION_CODE_na_count / n) * 100

# remove rows with NA 
df <- subset(df, JURISDICTION_CODE != "NA")

unique(df$JURISDICTION_CODE)

```

##### 2.2.8  Checkout NA values in remain columns
```{r}
unique(df$BORO)
unique(df$PRECINCT)
unique(df$STATISTICAL_MURDER_FLAG)

# Check the number of NA values
sum(is.na(df$OCCUR_TIME))  # Count NA in OCCUR_TIME
sum(is.na(df$OCCUR_DATE))  # Count NA in OCCUR_DATE

# print remain colunms after data clean
print(colnames(df))
```
### 3. Exploratory Data Analysis and Visualization

#### 3.1 Explore the case distribution in different characteristics of victim
```{r}
# Check out the victims
unique(df$VIC_AGE_GROUP)
df %>%
  filter(VIC_AGE_GROUP != "1022") ->
  df


# Define a function to analyze and visualize a category column.
analyze_category <- function(data, col_name){
    ggplot(data) +
    geom_bar(aes_string(y = col_name, fill = col_name))
}

# Analysis for victims
analyze_category(df, "VIC_AGE_GROUP")
analyze_category(df, "VIC_SEX")
analyze_category(df, "VIC_RACE")

```

#### 3.2 Geospatial anaylysis on this dataset
* Brooklyn and Bronx have the highest probability of incident occurrence
* And public safety resources should probably be directed toward these two areas.
```{r}
# Analysis geographically
analyze_category(df, "BORO")
```

#### 3.3 Temporal analysis on this dataset

```{r}
yearly_data <- df %>%
  mutate(year = as.integer(year(OCCUR_DATE))) %>%
  group_by(year) %>%
  summarize(n = n(), .groups = 'drop')

ggplot(yearly_data, aes(x = year, y = n)) +
  geom_line(color = "blue", size = 2) +
  labs(title = "Shooting Incidents in Years", 
       x = "Year", 
       y = "Number of Shootings") +
  scale_x_continuous(breaks = seq(min(yearly_data$year), max(yearly_data$year), by = 1)) +
  scale_y_continuous(limits = c(min(yearly_data$n), max(yearly_data$n)), 
                       breaks = seq(0, max(yearly_data$n), by = 300)) +
  annotate("text", x = max(yearly_data$year), y = 0, label = "Start", vjust = -1) 


```

* Let's check the plot of cases per year first. We can see that before 2019, the cases descent almost every year. But somehow, cases number increased sharply from 2019 to 2020.
* Maybe The Covid has some impact on it.





```{r}
monthly_data <- df %>%
  mutate(month = as.integer(month(OCCUR_DATE))) %>%
  group_by(month) %>%
  summarize(n = n(), .groups = 'drop')

ggplot(monthly_data, aes(x = month, y = n)) +
  geom_line(color = "blue", size = 2) +
  labs(title = "Shooting Incidents in Month", 
       x = "Month", 
       y = "Number of Shootings") +
  scale_x_continuous(breaks = seq(min(monthly_data$month), max(monthly_data$month), by = 1)) +
  scale_y_continuous(limits = c(min(monthly_data$n), max(monthly_data$n)), 
                       breaks = seq(0, max(monthly_data$n), by = 300)) +
  annotate("text", x = max(monthly_data$month), y = 0, label = "Start", vjust = -1) 

```

* Checking the plot of cases per month, the most shootings occur in July and August.
* There seems to be a correlation between the number of shootings and the temperature.


### 4. Modeling
```{r}
# Convert 'occur_date' to Date type and extract the year
df_t <- df %>%
  mutate(occur_date = as.Date(OCCUR_DATE, format="%m/%d/%Y"), 
         occur_year = lubridate::year(OCCUR_DATE))

# Group the data by borough and year to summarize the number of shooting incidents
data_grouped <- df_t %>%
  group_by(BORO, occur_year) %>%
  summarise(shooting_count = n()) %>%
  ungroup()

head(data_grouped)

```

```{r}
ggplot(data_grouped, aes(x = occur_year, y = shooting_count, color = BORO)) +
  geom_line() +
  labs(title = "Shooting Incidents by Borough over Time", 
       x = "Year", 
       y = "Number of Shootings") +
  theme_minimal()
```

```{r}
# Create a linear model to see trends over time
model <- lm(shooting_count ~ occur_year + BORO, data = data_grouped)

# Print the summary of the model
summary(model)
```
### 5. Conclusion
This report analyzed the NYPD Shooting Incident Data to explore trends in shooting incidents across various boroughs in New York City over time. The primary objective was to identify whether there are discernible trends in the number of shootings across boroughs and determine if certain boroughs are more prone to shooting incidents.

The analysis revealed that the boroughs of Brooklyn and the Bronx consistently experienced higher numbers of shootings compared to Manhattan, Queens, and Staten Island. Additionally, the number of incidents showed an overall decline after a peak in the early 2000s, which may reflect changes in policing strategies, community programs, or socio-economic factors. It also increase sharply from 2019 to 2020, which might caused by the Covid.  

#### Possible Sources of Bias

While this analysis offers valuable insights, several sources of bias must be considered. Firstly, the dataset may contain reporting biases, where certain incidents might not have been reported or properly recorded. Secondly, missing data, such as incomplete incident details or incorrect categorization of boroughs, could have skewed the results.

#### Personal Bias and Mitigation

As a researcher, my personal bias may stem from my prior knowledge of New York City and assumptions regarding which boroughs might have higher crime rates. To mitigate this bias, I relied solely on the data-driven trends, avoiding assumptions based on my preconceived notions of the city. Additionally, I conducted an objective and systematic analysis of all boroughs rather than focusing disproportionately on any specific borough.

#### Future Work

This analysis is limited by the scope of the available dataset, which focuses solely on recorded shooting incidents. Future analyses should consider integrating socio-economic data, police activity records, and crime reporting patterns to offer a more comprehensive understanding of the factors driving these incidents. 